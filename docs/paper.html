<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Paper - System Prompt Forensics</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&family=Fira+Code:wght@400;500&display=swap');
        body { font-family: 'Inter', sans-serif; }
        code { font-family: 'Fira Code', monospace; }
        .prose h1, .prose h2, .prose h3 { color: #0f172a; font-weight: 700; margin-top: 2em; margin-bottom: 1em; }
        .prose p { margin-bottom: 1.25em; line-height: 1.75; color: #334155; }
        .prose ul { list-style-type: disc; padding-left: 1.5em; margin-bottom: 1.25em; color: #334155; }
        .prose li { margin-bottom: 0.5em; }
        .prose strong { color: #1e293b; font-weight: 600; }
    </style>
    <!-- 100% privacy-first analytics -->
    <script async src="https://scripts.simpleanalyticscdn.com/latest.js"></script>
</head>
<body class="bg-slate-50 text-slate-900 leading-relaxed">
    <nav class="bg-slate-900 text-white py-3 px-6 sticky top-0 z-50 shadow-md">
        <div class="max-w-5xl mx-auto flex justify-between items-center">
            <a href="index.html" class="flex items-center gap-2 hover:text-indigo-300 transition-colors">
                <img src="images/logo_dark_hires.png" alt="System Prompt Forensics Logo" class="h-8 w-auto">
                <span class="font-bold tracking-tight">System Prompt Forensics</span>
            </a>
            <div class="flex gap-6 text-sm font-medium items-center">
                <a href="index.html" class="hover:text-indigo-300 transition-colors">Summary</a>
                <a href="paper.html" class="text-indigo-300 font-semibold">Paper</a>
                <a href="report.html" class="hover:text-indigo-300 transition-colors">Report</a>
                <a href="appendix.html" class="hover:text-indigo-300 transition-colors">Appendix</a>
                <a href="briefs.html" class="hover:text-indigo-300 transition-colors">Briefs</a>
                <a href="https://github.com/rmax-ai/system-prompts-forensics/" class="hover:text-indigo-300 transition-colors" title="GitHub Repository">
                    <svg class="h-5 w-5 fill-current" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
                </a>
            </div>
        </div>
    </nav>
    <div class="max-w-4xl mx-auto px-6 py-12 lg:py-24">
        <header class="mb-16 border-b border-slate-200 pb-8">
            <nav class="mb-8">
                <a href="index.html" class="text-indigo-600 hover:text-indigo-800 font-medium flex items-center gap-2">
                    <svg xmlns="http://www.w3.org/2000/svg" class="h-4 w-4" fill="none" viewBox="0 0 24 24" stroke="currentColor">
                        <path stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 19l-7-7m0 0l7-7m-7 7h18" />
                    </svg>
                    Back to Summary
                </a>
            </nav>
            <h1 class="text-4xl font-bold tracking-tight text-slate-900 mb-4">System Prompts as Governance Artifacts in AI Developer Tools: A Forensic Comparative Study</h1>
            <p class="text-xl text-slate-600 italic">Research Paper</p>
        </header>

        <main class="space-y-16">
            <!-- Abstract -->
            <section id="abstract" class="bg-white p-8 rounded-xl shadow-sm border border-slate-100">
                <h2 class="text-2xl font-semibold mb-6 text-slate-800">Abstract</h2>
                <div class="prose prose-slate max-w-none">
                    <p>System prompts for AI developer tools are typically treated as implementation details, yet they function as constitutive governance artifacts: they allocate authority between user intent and policy, bound permissible actions, constrain visibility into workspace state, and define correction and termination behavior. This paper presents a prompt-forensics study of system prompts used by IDE and CLI developer assistants across interaction modes. We normalize prompts into a common schema and compare them along invariant dimensions—authority boundaries, scope and visibility controls, tool mediation, and correction/termination logic—to characterize how prompt text encodes governance regimes. Across assistants, we identify recurring constitutional patterns: mode-tiered autonomy (mode as constitution), tool-mediated accountability (tools as the enforceable action surface), separation of capability from permission (tools may exist while outcomes are forbidden), state minimization as risk control, and conservative change doctrines that protect workspace integrity. We synthesize these recurring controls into Prompt Governance Primitives (PGPs): reusable, prompt-encoded structures that can be composed to build or audit agentic systems. These findings are relevant to applied AI safety and agent architecture because tool-mediated agents are increasingly deployed in real repositories and terminals, where governance failures manifest as workspace corruption, autonomy drift, instruction leakage, and tool abuse.</p>
                </div>
            </section>

            <!-- 1. Introduction -->
            <section id="introduction">
                <h2 class="text-2xl font-semibold mb-6 text-slate-800 border-l-4 border-indigo-500 pl-4">1. Introduction</h2>
                <div class="prose prose-slate max-w-none">
                    <p>AI developer assistants increasingly behave as tool-mediated agents: they can read files, search repositories, run commands, and sometimes modify working directories. The system prompt that governs such assistants does more than provide task instructions. In practice, it functions as an “invisible constitution” that defines who the assistant is, what it may do, what it must refuse, and when it must stop.</p>
                    <p>This paper treats system prompts as a distinct governance layer. Rather than evaluating outputs in isolation, we analyze the text-level control structures encoded in system prompts across developer assistants and modes. We ask:</p>
                    <ul>
                        <li>How do system prompts allocate authority and constrain autonomy in developer assistants?</li>
                        <li>What recurring, reusable governance structures appear across tools and modes?</li>
                        <li>How do prompt-level controls attempt to mitigate failure modes for tool-using agents?</li>
                    </ul>
                    <p><strong>Contributions:</strong></p>
                    <ul>
                        <li>A comparative, schema-based analysis of system prompts across IDE and CLI developer assistants and interaction modes.</li>
                        <li>A synthesis of recurring prompt-encoded controls into Prompt Governance Primitives (PGPs).</li>
                        <li>A risk-oriented mapping from governance primitives to failure classes for tool-mediated agents.</li>
                    </ul>
                </div>
            </section>

            <!-- 2. Related Work -->
            <section id="related-work">
                <h2 class="text-2xl font-semibold mb-6 text-slate-800 border-l-4 border-indigo-500 pl-4">2. Related Work</h2>
                <div class="prose prose-slate max-w-none">
                    <p>The idea that high-level principles can govern model behavior has a clear antecedent in Constitutional AI, which aligns models via an explicit set of principles used for self-critique and refusal behavior (Anthropic, 2022). While Constitutional AI focuses on training-time alignment and normative principles, this paper examines deployed governance encoded directly in system prompts of developer tools.</p>
                    <p>Prompt injection and tool-calling vulnerabilities show how tool mediation can be exploited when boundaries are unclear or insufficiently enforced (Wang et al., 2025). Architectural patterns for securing agentic systems emphasize isolation, mediation, and constrained action selection; such patterns motivate treating “tool boundaries” and “action selection rules” as first-class control points (Beurer-Kellner et al., 2025; Masood, 2025).</p>
                    <p>Industry and practitioner analyses have highlighted that system prompts can be extensive and operationally consequential, functioning as governance documents that specify refusal policies, tool-use rules, and style constraints (Sharma, 2025; Willison, 2025). OpenAI’s discussion of a shift from hard refusals toward “safe-completions” emphasizes how governance can be expressed as output-centric policies and prompt-level constraints rather than binary compliance (OpenAI, 2025). Comparative analyses of model prompts further suggest that system prompts encode different architectural priorities (Forte, 2025).</p>
                    <p>What prior work less directly examines is the cross-tool, cross-mode governance structure of system prompts in developer assistants specifically—how prompts implement tiered autonomy, action gating, and workspace-integrity safeguards across operational modes (planner, reviewer, executor, full-access agent). This paper addresses that gap via comparative prompt forensics.</p>
                </div>
            </section>

            <!-- 3. Methodology -->
            <section id="methodology">
                <h2 class="text-2xl font-semibold mb-6 text-slate-800 border-l-4 border-indigo-500 pl-4">3. Methodology: System Prompt Forensics</h2>
                <div class="prose prose-slate max-w-none">
                    <h3 class="text-xl font-bold mb-4">3.1 Collection, normalization, and analysis</h3>
                    <p>We treat each assistant’s system prompt(s) as a governance artifact and analyze them using a normalized system-prompt schema. Prompts are collected per assistant and per mode, then normalized into a common representational structure to enable comparison. Each assistant’s modes are treated as constitutional variants within a single governance regime.</p>

                    <h3 class="text-xl font-bold mb-4">3.2 Analytical dimensions</h3>
                    <p>Comparative analysis is performed structurally along invariant dimensions that recur across assistants:</p>
                    <ul>
                        <li><strong>Authority boundaries:</strong> who is the final arbiter (policy, user, model) and how that authority is delegated or constrained by mode.</li>
                        <li><strong>Scope and visibility:</strong> what the assistant may assume, what state is visible, and what persistence/memory is permitted.</li>
                        <li><strong>Tool mediation:</strong> which tools exist, how tool use is sequenced, and how side effects are gated.</li>
                        <li><strong>Correction and termination:</strong> self-checking, stop-and-ask triggers, and hard termination contracts.</li>
                    </ul>

                    <h3 class="text-xl font-bold mb-4">3.3 Validity under partial observability</h3>
                    <p>Prompt-level analysis does not guarantee runtime enforcement. However, system prompts are explicit declarations of intended governance and are often the only observable specification of decision rights, tool constraints, and refusal/termination contracts. As such, they are valid for identifying architectural patterns, comparing governance regimes, and extracting reusable control structures—even when implementation details remain opaque.</p>

                    <h3 class="text-xl font-bold mb-4">3.4 Use of AI Assistance</h3>
                    <p>This research was produced with significant AI assistance across the analysis and synthesis pipeline. GPT-5.2 was utilized for primary data analysis, research report generation, and the development of the technical appendix and paper synthesis. ChatGPT was employed for initial idea conception and the iterative refinement of research prompts. ChatGPT Deep Research was used to identify and verify relevant citation sources. Gemini 3 Flash (via GitHub Copilot extension in VS Code) was used for final editorial review and refinement.</p>
                </div>
            </section>

            <!-- 4. Comparative Analysis -->
            <section id="comparative-analysis">
                <h2 class="text-2xl font-semibold mb-6 text-slate-800 border-l-4 border-indigo-500 pl-4">4. Comparative Analysis of Developer Assistants</h2>
                <div class="prose prose-slate max-w-none">
                    <h3 class="text-xl font-bold mb-4">4.1 Assistants and modes under study</h3>
                    <p>The analysis covers multiple assistants and modes, including local software engineering agents with execution and review constitutions; terminal assistants with interactive and prompt-oriented variants; CLI assistants with plan-versus-build splits; and IDE assistants with ask/plan/agent tiers and varying sandbox and approval semantics.</p>

                    <h3 class="text-xl font-bold mb-4">4.2 Authority models: partitioned by mode</h3>
                    <p>Authority is consistently partitioned rather than monolithic:</p>
                    <ul>
                        <li><strong>Policy-supremacy regimes</strong> explicitly elevate policy as the final arbiter over user intent.</li>
                        <li><strong>User-consent and escalation regimes</strong> expand authority conditionally via approval gates or escalation pathways when sandbox constraints block actions.</li>
                        <li><strong>Agent-final regimes</strong> place final decision authority with the model in “full-access” settings, compensating with stronger stop conditions and non-interference rules.</li>
                        <li><strong>Role-narrowing regimes</strong> restrict permissible outputs despite tool availability (e.g., review modes that forbid fixes; plan modes that forbid implementation).</li>
                    </ul>
                    <p>Across these regimes, mode boundaries operate as constitutional contracts that reallocate decision rights and permissible side effects.</p>

                    <h3 class="text-xl font-bold mb-4">4.3 Scope and visibility: risk control via bounded context</h3>
                    <p>Prompts frequently treat scope and visibility as governance levers:</p>
                    <ul>
                        <li><strong>State minimization</strong> (statelessness or non-persistence) limits long-horizon autonomy and reduces accumulated implicit commitments.</li>
                        <li><strong>Mode-level memory policies</strong> treat retention as a first-class risk variable.</li>
                        <li><strong>Partial visibility disclosures</strong> (e.g., describing sandboxing/network/roots and open tabs without their contents) indicate deliberate control of what the assistant may claim.</li>
                        <li><strong>Repository-local overlays</strong> (workspace rules) extend governance without changing the core constitution.</li>
                    </ul>

                    <h3 class="text-xl font-bold mb-4">4.4 Tool mediation: procedural governance</h3>
                    <p>Tooling is the dominant enforcement surface. Prompts constrain tool invocation via procedural obligations:</p>
                    <ul>
                        <li>Side effects are routed through declared tools (shell, file edits, search, tasks, web/GitHub access).</li>
                        <li>Modes implement <strong>side-effect gating</strong> (read-only plan/ask; write/execute agent).</li>
                        <li>Some regimes mandate <strong>tool sequencing</strong> (e.g., staged planning workflows) and encode efficiency or legibility constraints via parallelization and specialized-tool preference.</li>
                        <li>Capability and permission are decoupled: tools may exist while specific outcomes are forbidden.</li>
                    </ul>

                    <h3 class="text-xl font-bold mb-4">4.5 Correction and termination</h3>
                    <p>Correction loops and termination logic act as backstops:</p>
                    <ul>
                        <li>Self-checking behaviors appear across assistants, shifting from “does it work?” in executor modes to “is it compliant?” in reviewer/planner modes.</li>
                        <li>Workspace-integrity protection is encoded as explicit stop-and-ask triggers when unexpected changes are detected.</li>
                        <li>Some modes terminate via strict output contracts (e.g., schema-conforming responses).</li>
                    </ul>
                </div>
            </section>

            <!-- 5. Prompt Governance Primitives -->
            <section id="pgps">
                <h2 class="text-2xl font-semibold mb-6 text-slate-800 border-l-4 border-indigo-500 pl-4">5. Prompt Governance Primitives (PGPs)</h2>
                <div class="prose prose-slate max-w-none">
                    <h3 class="text-xl font-bold mb-4">5.1 Definition and rationale</h3>
                    <p>We define a Prompt Governance Primitive (PGP) as a recurring, prompt-encoded control structure that allocates authority, bounds scope and visibility, mediates tool use, constrains outputs, and/or defines correction and termination behavior.</p>

                    <h3 class="text-xl font-bold mb-4">5.1.1 PGP Taxonomy Diagram</h3>
                    <div class="mermaid bg-white p-4 rounded-lg border border-slate-200 mb-8">
flowchart TB

  subgraph ROW1[" "]
    direction LR
    subgraph F1["Authority & Consent Escalation"]
      direction TB
      F1M1["approval gates"] --> PGP001["PGP-001"]
      F1M2["explicit commit/push consent"] --> PGP008["PGP-008"]
    end


    subgraph F2["Workspace Integrity & Change Control"]
      direction TB
      F2M1["circuit breakers"] --> PGP004["PGP-004"]
      F2M2["prohibit destructive git actions"] --> PGP005["PGP-005"]
      F2M3["do-not-revert doctrine"] --> PGP006["PGP-006"]
      F2M4["no-amend rule"] --> PGP007["PGP-007"]
    end

    subgraph F3["Mode & Autonomy Partitioning"]
      direction TB
      F3M1["approval_policy constraints"] --> PGP002["PGP-002"]
      F3M2["read-only planning phase"] --> PGP009["PGP-009"]
      F3M3["structured work sequencing"] --> PGP017["PGP-017"]
    end


  end

  subgraph ROW3[" "]
    direction LR
    subgraph F4["Scope, Visibility & Context Hygiene"]
      direction TB
      F4M1["conservative sandbox defaults"] --> PGP003["PGP-003"]
      F4M2["progressive disclosure"] --> PGP010["PGP-010"]
    end

    subgraph F5["Tool Mediation & Operational Discipline"]
      direction TB
      F5M1["parallel tool batching"] --> PGP011["PGP-011"]
      F5M2["read-before-edit"] --> PGP012["PGP-012"]
      F5M3["doc-grounding capability answers"] --> PGP013["PGP-013"]
    end

    subgraph F6["Output Contracts, Confidentiality & Refusals"]
      direction TB
      F6M1["instruction confidentiality"] --> PGP014["PGP-014"]
      F6M2["strict output schemas"] --> PGP016["PGP-016"]
      F6M3["malicious-code refusal"] --> PGP015["PGP-015"]
    end
  end
                    </div>

                    <p>PGPs qualify as “primitives” because they recur across assistants, compose into larger governance regimes, and can be referenced independently of any single prompt.</p>
                </div>
            </section>

            <!-- 6. Risk Mitigation -->
            <section id="risk-mitigation">
                <h2 class="text-2xl font-semibold mb-6 text-slate-800 border-l-4 border-indigo-500 pl-4">6. Risk Mitigation and Failure Modes</h2>
                <div class="prose prose-slate max-w-none">
                    <p>System prompts encode mitigations intended to address concrete operational risks for tool-mediated agents.</p>
                    <ul>
                        <li><strong>Autonomy drift:</strong> mitigated via mode partitioning (plan vs build), approval gates, and explicit stop conditions.</li>
                        <li><strong>Workspace corruption and unintended side effects:</strong> mitigated via conservative change doctrines, prohibitions on destructive git actions, read-before-edit rules, and stop-on-unexpected-change circuit breakers.</li>
                        <li><strong>Instruction leakage:</strong> mitigated via explicit confidentiality constraints that treat system instructions as non-disclosable.</li>
                        <li><strong>Tool abuse and prompt injection:</strong> mitigated indirectly via tool mediation and procedural constraints. This aligns with evidence that adversarial injection can manipulate tool-calling if mediation boundaries are weak or conflated with untrusted inputs (Wang et al., 2025).</li>
                        <li><strong>Hallucination and ungrounded claims:</strong> mitigated by requiring tool-grounded inspection and, in some regimes, consulting authoritative documentation tools for capability questions.</li>
                    </ul>
                    <p>These prompt-encoded controls should be understood as intended mitigations rather than guarantees; their effectiveness depends on runtime enforcement.</p>
                </div>
            </section>

            <!-- 7. Implications -->
            <section id="implications">
                <h2 class="text-2xl font-semibold mb-6 text-slate-800 border-l-4 border-indigo-500 pl-4">7. Implications for Agent Design</h2>
                <div class="prose prose-slate max-w-none">
                    <p>The comparative analysis suggests that system prompts already function as constitutions for developer assistants. This has implications for both engineering practice and governance:</p>
                    <ul>
                        <li><strong>Agent-first development:</strong> governance should be treated as an explicit architecture layer, not incidental prompt text. Mode boundaries can be used to allocate autonomy tiers and constrain side effects.</li>
                        <li><strong>Multi-agent systems:</strong> specialized roles (planner, reviewer, executor) can be separated into distinct constitutions with explicit capability-permission boundaries.</li>
                        <li><strong>Enterprise governance:</strong> prompt registries, audits, and versioning are natural complements to treating prompts as governance artifacts (VerityAI, 2025 as cited).</li>
                        <li><strong>Applied AI safety engineering:</strong> prompt-level primitives provide a composable vocabulary for designing tool-mediated agents that must operate conservatively in real repositories.</li>
                    </ul>
                </div>
            </section>

            <!-- 8. Limitations -->
            <section id="limitations">
                <h2 class="text-2xl font-semibold mb-6 text-slate-800 border-l-4 border-indigo-500 pl-4">8. Limitations</h2>
                <div class="prose prose-slate max-w-none">
                    <p>Prompt-level analysis cannot establish runtime enforcement fidelity. Some regimes reference external policy documents or repository-local governance layers whose content is not fully observable, and several controls are specified as rules without exposing detection mechanisms.</p>
                </div>
            </section>

            <!-- 9. Conclusion -->
            <section id="conclusion">
                <h2 class="text-2xl font-semibold mb-6 text-slate-800 border-l-4 border-indigo-500 pl-4">9. Conclusion</h2>
                <div class="prose prose-slate max-w-none">
                    <p>Across IDE and CLI developer assistants, system prompts operate as constitutional governance documents. Abstracting these structures into Prompt Governance Primitives (PGPs) provides a reusable vocabulary for building, auditing, and comparing tool-mediated agents.</p>
                </div>
            </section>

            <!-- 10. References -->
            <section id="references">
                <h2 class="text-2xl font-semibold mb-6 text-slate-800 border-l-4 border-indigo-500 pl-4">References</h2>
                <div class="prose prose-slate max-w-none text-sm">
                    <ul class="space-y-2 list-none pl-0">
                        <li>Anthropic. 2022. <em>Constitutional AI: Harmlessness from AI Feedback.</em> arXiv:2212.08073. <a href="https://arxiv.org/abs/2212.08073" class="text-indigo-600 hover:underline">https://arxiv.org/abs/2212.08073</a></li>
                        <li>Beurer-Kellner, L. et al. 2025. <em>Design Patterns for Securing LLM Agents against Prompt Injections.</em> arXiv:2506.08837. <a href="https://arxiv.org/abs/2506.08837" class="text-indigo-600 hover:underline">https://arxiv.org/abs/2506.08837</a></li>
                        <li>Forte, Tiago. 2025. <em>A Guide to the Claude 4 and ChatGPT 5 System Prompts.</em> Forte Labs. <a href="https://fortelabs.com/blog/a-guide-to-the-claude-4-and-chatgpt-5-system-prompts/" class="text-indigo-600 hover:underline">https://fortelabs.com/blog/a-guide-to-the-claude-4-and-chatgpt-5-system-prompts/</a></li>
                        <li>Masood, Adnan. 2025. <em>The Sandboxed Mind — Principled Isolation Patterns for Prompt‑Injection‑Resilient LLM Agents.</em> Medium. <a href="https://medium.com/@adnanmasood/the-sandboxed-mind-principled-isolation-patterns-for-prompt-injection-resilient-llm-agents-c14f1f5f8495" class="text-indigo-600 hover:underline">https://medium.com/@adnanmasood/the-sandboxed-mind-principled-isolation-patterns-for-prompt-injection-resilient-llm-agents-c14f1f5f8495</a></li>
                        <li>OpenAI. 2025. <em>From hard refusals to safe-completions: toward output-centric safety training.</em> <a href="https://openai.com/index/gpt-5-safe-completions/" class="text-indigo-600 hover:underline">https://openai.com/index/gpt-5-safe-completions/</a></li>
                        <li>Sharma, Tuhin. 2025. <em>Claude 4 System Prompts: Operational Blueprint and Strategic Implications.</em> Medium. <a href="https://medium.com/@tuhinsharma121/decoding-claude-4-system-prompts-operational-blueprint-and-strategic-implications-727294cf79c3" class="text-indigo-600 hover:underline">https://medium.com/@tuhinsharma121/decoding-claude-4-system-prompts-operational-blueprint-and-strategic-implications-727294cf79c3</a></li>
                        <li>VerityAI. 2025. <em>System Prompts as Critical Control Points: The New Frontier of AI Governance.</em> (As cited.) <a href="https://www.nucamp.co/blog/coding-bootcamp-united-kingdom-gbr-marketing-work-smarter-not-harder-top-5-ai-prompts-every-marketing-professional-in-united-kingdom-should-use-in-2025" class="text-indigo-600 hover:underline">https://www.nucamp.co/blog/coding-bootcamp-united-kingdom-gbr-marketing-work-smarter-not-harder-top-5-ai-prompts-every-marketing-professional-in-united-kingdom-should-use-in-2025</a></li>
                        <li>Wang, H. et al. 2025. <em>From Allies to Adversaries: Manipulating LLM Tool-Calling through Adversarial Injection.</em> arXiv:2412.10198. <a href="https://arxiv.org/abs/2412.10198" class="text-indigo-600 hover:underline">https://arxiv.org/abs/2412.10198</a></li>
                        <li>Willison, Simon. 2025. <em>Highlights from the Claude 4 system prompt.</em> <a href="https://simonwillison.net/2025/May/25/claude-4-system-prompt/" class="text-indigo-600 hover:underline">https://simonwillison.net/2025/May/25/claude-4-system-prompt/</a></li>
                    </ul>
                </div>
            </section>
        </main>

        <footer class="mt-24 pt-8 border-t border-slate-200 text-center text-slate-500 text-sm">
            <p>Author: R. Max Espinoza | <a href="https://github.com/rmax-ai/system-prompts-forensics/" class="text-indigo-600 hover:underline">GitHub Repository</a> | <a href="https://rmax.ai" class="text-indigo-600 hover:underline">rmax.ai</a></p>
            <p class="mt-2">Produced with AI assistance (GPT-5.2). See <a href="index.html#disclosure" class="text-indigo-600 hover:underline">Disclosure</a> for details.</p>
            <p class="mt-4">&copy; 2026 R. Max Espinoza. Content licensed CC BY 4.0. Code licensed MIT.</p>
            <p class="mt-2"><a href="paper.html" class="text-indigo-600 hover:underline">Research Paper</a> | <a href="report.html" class="text-indigo-600 hover:underline">Technical Report</a> | <a href="appendix.html" class="text-indigo-600 hover:underline">Appendix</a> | <a href="briefs.html" class="text-indigo-600 hover:underline">Briefs</a></p>
        </footer>
    </div>
</body>
</html>
